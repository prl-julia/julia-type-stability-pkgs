Stability.ModuleStatsPerInstanceRecord[Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "consecutive_view", true, true, 0, 1, "SubArray{Bool,1,CircularArrayBuffers.CircularArrayBuffer{Bool,1},Tuple{Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Bool,1},Array{Int64,1},Nothing,Nothing", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 111), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "send_to_device", false, false, 0, 1, "Any", "Val{:cpu},NamedTuple{(:state, :action, :reward, :terminal, :next_state),T} where T<:Tuple", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 13), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreExperimentStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#StopAfterStep#178", false, false, 2, 1, "Union{StopAfterStep{Nothing}, StopAfterStep{ProgressMeter.Progress}}", "Int64,Bool,Type{StopAfterStep},Int64", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/stop_conditions.jl", 49), Stability.ModuleStatsPerInstanceRecord("Flux", "glorot_uniform", false, false, 0, 1, "Any", "Random._GLOBAL_RNG,Integer,Integer", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/Flux/goUGu/src/utils.jl", 65), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 48), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "is_body_missing", true, true, 0, 1, "Bool", "Val{_A} where _A,Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 64), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "consecutive_view", true, true, 0, 1, "SubArray{Float32,1,CircularArrayBuffers.CircularArrayBuffer{Float32,1},Tuple{Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Float32,1},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 109), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostEpisodeStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", false, false, 1, 2, "Any", "Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 40), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 3, 1, "Nothing", "Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostEpisodeStage", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl", 128), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "send_to_device", false, false, 0, 1, "Any", "Val{:cpu},NamedTuple{(:state, :action, :reward, :terminal, :next_state),Tuple{Array{Float32,2},Array{Int64,1},Array{Float32,1},Array{Bool,1},Array{Float32,2}}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 13), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningZoo", "update!", true, false, 4, 1, "Nothing", "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},NamedTuple{(:state, :action, :reward, :terminal, :next_state),T} where T<:Tuple", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl", 69), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "sample", false, false, 0, 1, "Tuple{Array{Int64,1},Any}", "StableRNGs.LehmerRNG,Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},BatchSampler", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl", 71), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "@api", true, false, 0, 1, "Expr", "LineNumberNode,Module,Any", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 11), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "select_last_dim", true, true, 0, 1, "SubArray{Int64,1,CircularArrayBuffers.CircularArrayBuffer{Int64,1},Tuple{Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Int64,1},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 18), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 0, 1, "Nothing", "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreEpisodeStage", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/abstract_learner.jl", 22), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#TimePerStep#166", true, true, 0, 1, "TimePerStep", "Int64,Type{TimePerStep}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 228), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "send_to_device", false, false, 0, 1, "Any", "Val{:cpu},Array{Float32,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 13), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "ActionStyle", true, true, 0, 1, "MinimalActionSet", "Type{CartPoleEnv{Float32,StableRNGs.LehmerRNG}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/interface.jl", 323), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreEpisodeStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#consecutive_view#5", false, false, 0, 1, "Any", "Nothing,Nothing,typeof(consecutive_view),CircularArrayBuffers.CircularArrayBuffer,Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 109), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#getindex#50", false, false, 0, 1, "CircularArrayBuffers.CircularArrayBuffer", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(getindex),Trajectory{var\"#s57\"} where var\"#s57\"<:(NamedTuple{(:state, :action, :reward, :terminal),var\"#s16\"} where var\"#s16\"<:(Tuple{var\"#s15\",var\"#s14\",var\"#s12\",var\"#s84\"} where var\"#s84\"<:CircularArrayBuffers.CircularArrayBuffer where var\"#s12\"<:CircularArrayBuffers.CircularArrayBuffer where var\"#s14\"<:CircularArrayBuffers.CircularArrayBuffer where var\"#s15\"<:CircularArrayBuffers.CircularArrayBuffer)),Symbol", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl", 17), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "select_last_dim", true, true, 0, 1, "SubArray{Float32,2,CircularArrayBuffers.CircularArrayBuffer{Float32,2},Tuple{Base.Slice{Base.OneTo{Int64}},Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Float32,2},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 18), Stability.ModuleStatsPerInstanceRecord("SparseArrays", "copyto!", false, false, 0, 1, "Array{_A,2} where _A", "Array{_A,2} where _A,SparseArrays.AbstractSparseMatrixCSC", "/build/source/usr/share/julia/stdlib/v1.5/SparseArrays/src/sparsematrix.jl", 357), Stability.ModuleStatsPerInstanceRecord("StableRNGs", "seed!", true, false, 4, 1, "StableRNGs.LehmerRNG", "StableRNGs.LehmerRNG,Int64", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/StableRNGs/uwZ1I/src/StableRNGs.jl", 39), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostExperimentStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#NeuralNetworkApproximator#112", false, false, 0, 1, "NeuralNetworkApproximator{_A,_B} where _B where _A", "Any,Any,Type{NeuralNetworkApproximator}", "util.jl", 448), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "include", false, false, 0, 1, "Any", "String", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/ReinforcementLearningCore.jl", 1), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#114", false, false, 0, 1, "Any", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},Any", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/approximators/neural_network_approximator.jl", 23), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 2, 2, "Nothing", "Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreEpisodeStage", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl", 95), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#params#117", true, true, 0, 1, "Zygote.Params", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(Flux.params),NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl", 17), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "select_last_dim", true, true, 0, 1, "SubArray{Bool,1,CircularArrayBuffers.CircularArrayBuffer{Bool,1},Tuple{Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Bool,1},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 18), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "get_ϵ", true, true, 1, 2, "Float64", "EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG},Int64", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/explorers/epsilon_greedy_explorer.jl", 88), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "select_last_frame", true, true, 0, 1, "Int64", "CircularArrayBuffers.CircularArrayBuffer{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 21), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "send_to_device", false, false, 0, 1, "Any", "Val{:gpu},Array{Float32,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 14), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", false, false, 0, 1, "Any", "Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 22), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "is_body_missing", true, true, 0, 1, "Bool", "Val{:function},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 64), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearning", "eval", false, false, 0, 1, "Any", "Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearning/OfIXm/src/ReinforcementLearning.jl", 1), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "reset!", true, false, 0, 1, "Nothing", "CartPoleEnv{T<:Number,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl", 76), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "__init__", false, false, 4, 2, "Any", "", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/ReinforcementLearningEnvironments.jl", 19), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "Val{:const},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 59), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 0, 1, "Nothing", "QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreActStage", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/q_based_policy.jl", 57), Stability.ModuleStatsPerInstanceRecord("Random.DSFMT", "copy", true, true, 0, 1, "Random.DSFMT.DSFMT_state", "Random.DSFMT.DSFMT_state", "/build/source/usr/share/julia/stdlib/v1.5/Random/src/DSFMT.jl", 38), Stability.ModuleStatsPerInstanceRecord("LinearAlgebra", "copyto!", true, true, 4, 1, "Array{Float64,2}", "Array{Float64,2},UnitRange{Int64},UnitRange{Int64},Char,Array{Float64,2},UnitRange{Int64},UnitRange{Int64}", "/build/source/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl", 607), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "_run", true, false, 7, 1, "ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}}", "Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},StopAfterStep{ProgressMeter.Progress},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/run.jl", 16), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 0, 1, "Nothing", "QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreEpisodeStage", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/q_based_policy.jl", 57), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "handle", false, false, 0, 1, "Tuple{Any,Bool}", "Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 43), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "eval", false, false, 0, 1, "Any", "Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/ReinforcementLearningEnvironments.jl", 1), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "@E_cmd", false, false, 0, 1, "Any", "LineNumberNode,Module,Any", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/experiment.jl", 24), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#QBasedPolicy#136", true, true, 0, 1, "QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}}", "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG},Type{QBasedPolicy}", "util.jl", 448), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 0, 1, "Nothing", "NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},Zygote.Grads", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/approximators/neural_network_approximator.jl", 33), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", false, false, 0, 1, "Any", "Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 22), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#NeuralNetworkApproximator#112", true, true, 0, 1, "NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM}", "Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM,Type{NeuralNetworkApproximator}", "util.jl", 448), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "select_last_dim", true, true, 0, 1, "SubArray{Float32,1,CircularArrayBuffers.CircularArrayBuffer{Float32,1},Tuple{Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Float32,1},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 18), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "Val{:<:},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 53), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#EpsilonGreedyExplorer#127", false, false, 0, 1, "EpsilonGreedyExplorer{_A,_B,StableRNGs.LehmerRNG} where _B where _A", "Float64,Symbol,Float64,Int64,Int64,Int64,Bool,Bool,StableRNGs.LehmerRNG,Type{EpsilonGreedyExplorer}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/explorers/epsilon_greedy_explorer.jl", 53), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "reset!", true, false, 0, 1, "Nothing", "CartPoleEnv{_A,StableRNGs.LehmerRNG} where _A", "none", 0), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "Val{:struct},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 58), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#consecutive_view#5", true, true, 0, 1, "SubArray{Bool,1,CircularArrayBuffers.CircularArrayBuffer{Bool,1},Tuple{Array{Int64,1}},false}", "Nothing,Nothing,typeof(consecutive_view),CircularArrayBuffers.CircularArrayBuffer{Bool,1},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 109), Stability.ModuleStatsPerInstanceRecord("CUDA", "device", true, true, 3, 1, "CUDA.CuDevice", "", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/CUDA/mbPFj/src/state.jl", 224), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", true, true, 0, 1, "Nothing", "NamedTuple", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 40), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "Val{:call},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 51), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", true, false, 0, 1, "Val{:cpu}", "Array", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 25), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#DoEveryNEpisode#171", true, true, 0, 1, "DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}", "PostEpisodeStage,Type{DoEveryNEpisode},ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode},Int64,Int64", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 270), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "send_to_host", false, false, 0, 1, "Any", "Array{Float32,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 11), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostActStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#TotalRewardPerEpisode#161", true, true, 0, 1, "TotalRewardPerEpisode", "Array{Float64,1},Float64,Type{TotalRewardPerEpisode}", "util.jl", 438), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#keys#51", true, true, 0, 1, "NTuple{4,Symbol}", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(keys),Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl", 17), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "consecutive_view", true, true, 0, 1, "SubArray{Float32,2,CircularArrayBuffers.CircularArrayBuffer{Float32,2},Tuple{Base.Slice{Base.OneTo{Int64}},Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Float32,2},Array{Int64,1},Nothing,Nothing", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 111), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", false, false, 1, 2, "Any", "Tuple{Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 40), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningZoo", "update!", true, false, 4, 1, "Nothing", "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},NamedTuple{(:state, :action, :reward, :terminal, :next_state),Tuple{Array{Float32,2},Array{Int64,1},Array{Float32,1},Array{Bool,1},Array{Float32,2}}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl", 69), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "action_space", true, true, 0, 1, "Base.OneTo{Int64}", "CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl", 84), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", true, true, 1, 2, "Union{Val{:cpu}, Nothing}", "NamedTuple{(:b, :σ),Tuple{Array{Float32,1},typeof(NNlib.relu)}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 40), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "fetch!", false, false, 1, 2, "Any", "BatchSampler{(:state, :action, :reward, :terminal, :next_state)},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl", 88), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", true, true, 0, 1, "Val{:cpu}", "Array{Float32,2}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 25), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "interfacem", true, false, 2, 3, "Expr", "Module,LineNumberNode,Expr,Array{Any,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 23), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "check", true, true, 0, 1, "Nothing", "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/run.jl", 14), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "send_to_host", false, false, 0, 1, "Any", "Any", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 11), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningZoo", "include", false, false, 0, 1, "Any", "String", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/ReinforcementLearningZoo.jl", 1), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#Agent#102", false, false, 0, 1, "Agent", "Any,Any,Type{Agent}", "util.jl", 448), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreActStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},Any", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 0, 1, "Nothing", "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostActStage", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/abstract_learner.jl", 22), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreEpisodeStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", true, true, 0, 1, "Nothing", "Tuple{}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 26), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningZoo", "update!", true, false, 1, 2, "Nothing", "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl", 62), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, false, 3, 1, "Nothing", "Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreActStage,Any", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl", 110), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "consecutive_view", false, false, 0, 1, "Any", "CircularArrayBuffers.CircularArrayBuffer,Array{Int64,1},Nothing,Nothing", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 111), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", false, false, 1, 2, "Any", "Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 40), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "is_body_missing", true, true, 0, 1, "Bool", "Val{:abstract},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 64), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "state", true, true, 0, 1, "Array{Float32,1}", "CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl", 97), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 0, 1, "Nothing", "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreActStage", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/abstract_learner.jl", 29), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", false, false, 1, 2, "Any", "NamedTuple{(:Q,),Tuple{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM}}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 40), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "reward", true, true, 1, 2, "Float32", "CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl", 95), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningZoo", "#Experiment#368", true, false, 1, 1, "Experiment", "Int64,Nothing,Type{Experiment},Val{:JuliaRL},Val{:BasicDQN},Val{:CartPole},Nothing", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/experiments/rl_envs/JuliaRL_BasicDQN_CartPole.jl", 1), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningZoo", "#BasicDQNLearner#51", false, false, 0, 1, "BasicDQNLearner{_A,_B,_C} where _C where _B where _A", "Any,Any,Float32,Any,Any,Any,Type{BasicDQNLearner}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl", 43), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "is_terminated", true, true, 0, 1, "Bool", "CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl", 96), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "send_to_device", false, false, 0, 1, "Any", "Val{:cpu},Any", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 13), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#device#118", false, false, 0, 1, "Any", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(device),NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl", 17), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "legal_action_space_mask", true, true, 0, 0, "Union{}", "CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "none", 0), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "consecutive_view", true, true, 0, 1, "SubArray{Bool,1,CircularArrayBuffers.CircularArrayBuffer{Bool,1},Tuple{Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Bool,1},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 109), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "legal_action_space_mask", true, true, 0, 0, "Union{}", "CartPoleEnv{Float32,StableRNGs.LehmerRNG},DefaultPlayer", "none", 0), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostExperimentStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "fetch!", false, false, 1, 2, "Any", "BatchSampler{(:state, :action, :reward, :terminal, :next_state)},Trajectory{var\"#s57\"} where var\"#s57\"<:(NamedTuple{(:state, :action, :reward, :terminal),var\"#s16\"} where var\"#s16\"<:(Tuple{var\"#s15\",var\"#s14\",var\"#s12\",var\"#s84\"} where var\"#s84\"<:CircularArrayBuffers.CircularArrayBuffer where var\"#s12\"<:CircularArrayBuffers.CircularArrayBuffer where var\"#s14\"<:CircularArrayBuffers.CircularArrayBuffer where var\"#s15\"<:CircularArrayBuffers.CircularArrayBuffer)),Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl", 88), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "Val{:curly},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 54), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "consecutive_view", true, true, 0, 1, "SubArray{Int64,1,CircularArrayBuffers.CircularArrayBuffer{Int64,1},Tuple{Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Int64,1},Array{Int64,1},Nothing,Nothing", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 111), Stability.ModuleStatsPerInstanceRecord("CommonRLInterface", "include", false, false, 0, 1, "Any", "String", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/CommonRLInterface/aRBRq/src/CommonRLInterface.jl", 1), Stability.ModuleStatsPerInstanceRecord("Flux", "glorot_uniform", false, false, 0, 1, "Any", "StableRNGs.LehmerRNG,Int64,Vararg{Int64,N} where N", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/Flux/goUGu/src/utils.jl", 65), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "get_ϵ", true, true, 1, 2, "Float64", "EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/explorers/epsilon_greedy_explorer.jl", 98), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "sample", false, false, 0, 1, "Tuple{Array{Int64,1},Any}", "StableRNGs.LehmerRNG,Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},BatchSampler{(:state, :action, :reward, :terminal, :next_state)}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl", 71), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#76", true, true, 0, 1, "BatchSampler{(:state, :action, :reward, :terminal, :next_state)}", "Nothing,Random._GLOBAL_RNG,Type{BatchSampler{(:state, :action, :reward, :terminal, :next_state)}},Int64", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl", 65), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "consecutive_view", true, true, 0, 1, "SubArray{Int64,1,CircularArrayBuffers.CircularArrayBuffer{Int64,1},Tuple{Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Int64,1},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 109), Stability.ModuleStatsPerInstanceRecord("Flux", "glorot_uniform", true, true, 0, 1, "Array{Float32,2}", "StableRNGs.LehmerRNG,Int64,Int64", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/Flux/goUGu/src/utils.jl", 65), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningZoo", "eval", false, false, 0, 1, "Any", "Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/ReinforcementLearningZoo.jl", 1), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "Val{:(=)},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 50), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#consecutive_view#5", true, true, 0, 1, "SubArray{Int64,1,CircularArrayBuffers.CircularArrayBuffer{Int64,1},Tuple{Array{Int64,1}},false}", "Nothing,Nothing,typeof(consecutive_view),CircularArrayBuffers.CircularArrayBuffer{Int64,1},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 109), Stability.ModuleStatsPerInstanceRecord("Flux", "glorot_uniform", false, false, 0, 1, "Any", "Integer,Integer", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/Flux/goUGu/src/utils.jl", 66), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "check", true, true, 0, 1, "Nothing", "EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG},CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/run.jl", 14), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#getindex#50", false, false, 0, 1, "CircularArrayBuffers.CircularArrayBuffer", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(getindex),Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},Symbol", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl", 17), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", true, true, 0, 1, "Val{:cpu}", "Array{Float32,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 25), Stability.ModuleStatsPerInstanceRecord("LoopVectorization", "copyto!", true, true, 2, 1, "Union{Nothing, LoopVectorization.Operation}", "LoopVectorization.LoopSet,Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/LoopVectorization/s8Gx6/src/constructors.jl", 13), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 0, 1, "Nothing", "QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostEpisodeStage", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/q_based_policy.jl", 57), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "select_last_dim", false, false, 0, 1, "Any", "CircularArrayBuffers.CircularArrayBuffer,Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 18), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "check", true, true, 2, 1, "Nothing", "Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl", 26), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "#CartPoleEnv#40", false, false, 0, 1, "CartPoleEnv{_A,StableRNGs.LehmerRNG} where _A", "Type{T} where T,Float64,Float64,Float64,Float64,Float64,Int64,Float64,StableRNGs.LehmerRNG,Type{CartPoleEnv}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#114", true, true, 0, 1, "Array{Float32,2}", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},Array{Float32,2}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/approximators/neural_network_approximator.jl", 23), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "is_body_missing", true, true, 0, 1, "Bool", "Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 63), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostActStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "select_last_dim", true, true, 0, 1, "Int64", "CircularArrayBuffers.CircularArrayBuffer{Int64,1},Int64", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 18), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "include", false, false, 0, 1, "Any", "String", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/ReinforcementLearningBase.jl", 1), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "is_body_missing", true, true, 0, 1, "Bool", "Val{:const},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 64), Stability.ModuleStatsPerInstanceRecord("Pkg.Types", "copy", true, true, 0, 1, "Pkg.Types.VersionSpec", "Pkg.Types.VersionSpec", "/build/source/usr/share/julia/stdlib/v1.5/Pkg/src/versions.jl", 230), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningZoo", "#BasicDQNLearner#51", true, true, 0, 1, "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG}", "NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),Float32,Int64,Int64,StableRNGs.LehmerRNG,Type{BasicDQNLearner}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl", 43), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 1, 1, "CircularArrayBuffers.CircularArrayBuffer{Bool,1}", "Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostActStage", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl", 153), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 0, 1, "Nothing", "QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostActStage", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/q_based_policy.jl", 57), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "consecutive_view", true, true, 0, 1, "SubArray{Float32,2,CircularArrayBuffers.CircularArrayBuffer{Float32,2},Tuple{Base.Slice{Base.OneTo{Int64}},Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Float32,2},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 109), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreActStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#Agent#102", true, true, 0, 1, "Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}}", "QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},Type{Agent}", "util.jl", 448), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "send_to_device", false, false, 0, 1, "Any", "Val{:gpu},NamedTuple{(:state, :action, :reward, :terminal, :next_state),Tuple{Array{Float32,2},Array{Int64,1},Array{Float32,1},Array{Bool,1},Array{Float32,2}}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 14), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#CircularArrayTrajectory#52", false, false, 0, 1, "Trajectory{_A} where _A", "Int64,Base.Iterators.Pairs{Symbol,Pair{DataType,Tuple{}},Tuple{Symbol,Symbol},NamedTuple{(:reward, :terminal),Tuple{Pair{DataType,Tuple{}},Pair{DataType,Tuple{}}}}},typeof(CircularArrayTrajectory)", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory.jl", 44), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "is_body_missing", true, true, 0, 1, "Bool", "Val{:call},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 65), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", false, false, 0, 1, "Any", "NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl", 17), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostEpisodeStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "QuoteNode", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 47), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 0, 1, "Nothing", "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostEpisodeStage", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/abstract_learner.jl", 22), Stability.ModuleStatsPerInstanceRecord("CommonRLInterface.Wrappers", "include", false, false, 0, 1, "Any", "String", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/CommonRLInterface/aRBRq/src/wrappers.jl", 1), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", true, true, 0, 1, "Nothing", "typeof(NNlib.relu)", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 23), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "#CartPoleEnv#40", false, false, 0, 1, "CartPoleEnv{_A,StableRNGs.LehmerRNG} where _A", "DataType,Float64,Float64,Float64,Float64,Float64,Int64,Float64,StableRNGs.LehmerRNG,Type{CartPoleEnv}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "check", true, true, 3, 1, "Nothing", "QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/q_based_policy.jl", 66), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningZoo", "__init__", false, false, 1, 2, "Any", "", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/ReinforcementLearningZoo.jl", 36), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "Val{:(::)},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 52), Stability.ModuleStatsPerInstanceRecord("CommonRLInterface.Wrappers", "@forward_to_wrapped", true, false, 0, 1, "Expr", "LineNumberNode,Module,Any", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/CommonRLInterface/aRBRq/src/wrappers.jl", 42), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", false, false, 1, 2, "Any", "NamedTuple{(:Q,),_A} where _A<:Tuple", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 40), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", false, false, 0, 1, "Any", "Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 22), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "consecutive_view", true, true, 0, 1, "SubArray{Float32,1,CircularArrayBuffers.CircularArrayBuffer{Float32,1},Tuple{Array{Int64,1}},false}", "CircularArrayBuffers.CircularArrayBuffer{Float32,1},Array{Int64,1},Nothing,Nothing", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 111), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#CircularArrayTrajectory#52", false, false, 0, 1, "Trajectory{_A} where _A", "Int64,Base.Iterators.Pairs{Symbol,Pair{DataType,B} where B,Tuple{Symbol,Symbol},NamedTuple{(:state, :action),Tuple{Pair{DataType,Tuple{Int64}},Pair{DataType,Tuple{}}}}},typeof(CircularArrayTrajectory)", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory.jl", 44), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "include", false, false, 0, 1, "Any", "String", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/ReinforcementLearningEnvironments.jl", 1), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "@multi_agent_env_api", true, false, 0, 1, "Expr", "LineNumberNode,Module,Any", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 19), Stability.ModuleStatsPerInstanceRecord("CommonRLInterface.Wrappers", "@quick_forward", true, false, 0, 1, "Expr", "LineNumberNode,Module,Any", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/CommonRLInterface/aRBRq/src/quick_wrapper.jl", 32), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "update!", true, true, 3, 1, "Nothing", "Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreActStage,Int64", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl", 110), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningZoo", "update!", true, false, 1, 2, "Nothing", "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},AbstractTrajectory", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl", 62), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", false, false, 0, 1, "Any", "BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 22), Stability.ModuleStatsPerInstanceRecord("Flux", "glorot_uniform", true, true, 0, 1, "Flux.var\"#1#2\"{StableRNGs.LehmerRNG}", "StableRNGs.LehmerRNG", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/Flux/goUGu/src/utils.jl", 67), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "sample", false, false, 0, 1, "Tuple{Any,Any}", "StableRNGs.LehmerRNG,AbstractTrajectory,BatchSampler", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl", 71), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "ActionStyle", true, true, 0, 1, "MinimalActionSet", "CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/interface.jl", 322), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "#CartPoleEnv#40", true, true, 0, 1, "CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "Type{Float32},Float64,Float64,Float64,Float64,Float64,Int64,Float64,StableRNGs.LehmerRNG,Type{CartPoleEnv}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#CircularArraySARTTrajectory#58", false, false, 0, 1, "Trajectory{_A} where _A", "Int64,Pair{DataType,Tuple{Int64}},Pair{DataType,Tuple{}},Pair{DataType,Tuple{}},Pair{DataType,Tuple{}},Type{Trajectory{var\"#s57\"} where var\"#s57\"<:(NamedTuple{(:state, :action, :reward, :terminal),var\"#s16\"} where var\"#s16\"<:(Tuple{var\"#s15\",var\"#s14\",var\"#s12\",var\"#s84\"} where var\"#s84\"<:CircularArrayBuffers.CircularArrayBuffer where var\"#s12\"<:CircularArrayBuffers.CircularArrayBuffer where var\"#s14\"<:CircularArrayBuffers.CircularArrayBuffer where var\"#s15\"<:CircularArrayBuffers.CircularArrayBuffer))}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory.jl", 76), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#consecutive_view#5", true, true, 0, 1, "SubArray{Float32,1,CircularArrayBuffers.CircularArrayBuffer{Float32,1},Tuple{Array{Int64,1}},false}", "Nothing,Nothing,typeof(consecutive_view),CircularArrayBuffers.CircularArrayBuffer{Float32,1},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 109), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "is_body_missing", true, true, 0, 1, "Bool", "Val{:struct},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 64), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", true, true, 1, 2, "Union{Val{:cpu}, Nothing}", "NamedTuple{(:W, :b, :σ),Tuple{Array{Float32,2},Array{Float32,1},typeof(NNlib.relu)}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 40), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#157", true, false, 4, 1, "Nothing", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var\"#369#373\"{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var\"#371#375\"{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreExperimentStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl", 45), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", true, true, 0, 1, "Nothing", "NamedTuple{(),Tuple{}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 27), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "@env_api", true, false, 0, 1, "Expr", "LineNumberNode,Module,Any", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 15), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningEnvironments", "reset!", true, true, 0, 1, "Nothing", "CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl", 76), Stability.ModuleStatsPerInstanceRecord("AbstractTrees", "include", false, false, 0, 1, "Any", "String", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/AbstractTrees/oHb1F/src/AbstractTrees.jl", 7), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", true, true, 0, 1, "Symbol", "Symbol", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 46), Stability.ModuleStatsPerInstanceRecord("LinearAlgebra", "copyto!", true, true, 4, 1, "Array{Float32,2}", "Array{Float32,2},UnitRange{Int64},UnitRange{Int64},Char,Array{Float32,2},UnitRange{Int64},UnitRange{Int64}", "/build/source/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl", 607), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#_#114", true, true, 0, 1, "Array{Float32,1}", "Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},Array{Float32,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/approximators/neural_network_approximator.jl", 23), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "Val{:where},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 56), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "current_player", true, true, 0, 1, "DefaultPlayer", "CartPoleEnv{Float32,StableRNGs.LehmerRNG}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/interface.jl", 476), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", true, true, 1, 2, "Union{Val{:cpu}, Nothing}", "NamedTuple{(:W, :b, :σ),Tuple{Array{Float32,2},Array{Float32,1},typeof(identity)}}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 40), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "Val{:function},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 57), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "is_body_missing", true, true, 0, 1, "Bool", "Val{:(=)},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 64), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "device", true, true, 0, 1, "Nothing", "typeof(identity)", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl", 23), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "consecutive_view", false, false, 0, 1, "Any", "CircularArrayBuffers.CircularArrayBuffer,Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 109), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningBase", "extract_name", false, false, 0, 1, "Any", "Val{:abstract},Expr", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl", 49), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#consecutive_view#5", true, true, 0, 1, "SubArray{Float32,2,CircularArrayBuffers.CircularArrayBuffer{Float32,2},Tuple{Base.Slice{Base.OneTo{Int64}},Array{Int64,1}},false}", "Nothing,Nothing,typeof(consecutive_view),CircularArrayBuffers.CircularArrayBuffer{Float32,2},Array{Int64,1}", "/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl", 109), Stability.ModuleStatsPerInstanceRecord("ReinforcementLearningCore", "#QBasedPolicy#136", false, false, 0, 1, "QBasedPolicy", "Any,Any,Type{QBasedPolicy}", "util.jl", 448)]
