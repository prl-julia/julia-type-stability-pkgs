modl,funcname,stable,grounded,gotos,returns,rettype,intypes,file,line
ReinforcementLearningCore,consecutive_view,true,true,0,1,"SubArray{Bool,1,CircularArrayBuffers.CircularArrayBuffer{Bool,1},Tuple{Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Bool,1},Array{Int64,1},Nothing,Nothing",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,111
ReinforcementLearningCore,send_to_device,false,false,0,1,Any,"Val{:cpu},NamedTuple{(:state, :action, :reward, :terminal, :next_state),T} where T<:Tuple",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,13
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreExperimentStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningCore,#StopAfterStep#178,false,false,2,1,"Union{StopAfterStep{Nothing}, StopAfterStep{ProgressMeter.Progress}}","Int64,Bool,Type{StopAfterStep},Int64",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/stop_conditions.jl,49
Flux,glorot_uniform,false,false,0,1,Any,"Random._GLOBAL_RNG,Integer,Integer",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/Flux/goUGu/src/utils.jl,65
ReinforcementLearningBase,extract_name,false,false,0,1,Any,Expr,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,48
ReinforcementLearningBase,is_body_missing,true,true,0,1,Bool,"Val{_A} where _A,Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,64
ReinforcementLearningCore,consecutive_view,true,true,0,1,"SubArray{Float32,1,CircularArrayBuffers.CircularArrayBuffer{Float32,1},Tuple{Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Float32,1},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,109
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostEpisodeStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningCore,device,false,false,1,2,Any,"Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,40
ReinforcementLearningCore,update!,true,true,3,1,Nothing,"Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostEpisodeStage",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl,128
ReinforcementLearningCore,send_to_device,false,false,0,1,Any,"Val{:cpu},NamedTuple{(:state, :action, :reward, :terminal, :next_state),Tuple{Array{Float32,2},Array{Int64,1},Array{Float32,1},Array{Bool,1},Array{Float32,2}}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,13
ReinforcementLearningZoo,update!,true,false,4,1,Nothing,"BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},NamedTuple{(:state, :action, :reward, :terminal, :next_state),T} where T<:Tuple",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl,69
ReinforcementLearningCore,sample,false,false,0,1,"Tuple{Array{Int64,1},Any}","StableRNGs.LehmerRNG,Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},BatchSampler",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl,71
ReinforcementLearningBase,@api,true,false,0,1,Expr,"LineNumberNode,Module,Any",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,11
ReinforcementLearningCore,select_last_dim,true,true,0,1,"SubArray{Int64,1,CircularArrayBuffers.CircularArrayBuffer{Int64,1},Tuple{Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Int64,1},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,18
ReinforcementLearningCore,update!,true,true,0,1,Nothing,"BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreEpisodeStage",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/abstract_learner.jl,22
ReinforcementLearningCore,#TimePerStep#166,true,true,0,1,TimePerStep,"Int64,Type{TimePerStep}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,228
ReinforcementLearningCore,send_to_device,false,false,0,1,Any,"Val{:cpu},Array{Float32,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,13
ReinforcementLearningBase,ActionStyle,true,true,0,1,MinimalActionSet,"Type{CartPoleEnv{Float32,StableRNGs.LehmerRNG}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/interface.jl,323
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreEpisodeStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningCore,#consecutive_view#5,false,false,0,1,Any,"Nothing,Nothing,typeof(consecutive_view),CircularArrayBuffers.CircularArrayBuffer,Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,109
ReinforcementLearningCore,#getindex#50,false,false,0,1,CircularArrayBuffers.CircularArrayBuffer,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(getindex),Trajectory{var""#s57""} where var""#s57""<:(NamedTuple{(:state, :action, :reward, :terminal),var""#s16""} where var""#s16""<:(Tuple{var""#s15"",var""#s14"",var""#s12"",var""#s84""} where var""#s84""<:CircularArrayBuffers.CircularArrayBuffer where var""#s12""<:CircularArrayBuffers.CircularArrayBuffer where var""#s14""<:CircularArrayBuffers.CircularArrayBuffer where var""#s15""<:CircularArrayBuffers.CircularArrayBuffer)),Symbol",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl,17
ReinforcementLearningCore,select_last_dim,true,true,0,1,"SubArray{Float32,2,CircularArrayBuffers.CircularArrayBuffer{Float32,2},Tuple{Base.Slice{Base.OneTo{Int64}},Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Float32,2},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,18
SparseArrays,copyto!,false,false,0,1,"Array{_A,2} where _A","Array{_A,2} where _A,SparseArrays.AbstractSparseMatrixCSC",/build/source/usr/share/julia/stdlib/v1.5/SparseArrays/src/sparsematrix.jl,357
StableRNGs,seed!,true,false,4,1,StableRNGs.LehmerRNG,"StableRNGs.LehmerRNG,Int64",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/StableRNGs/uwZ1I/src/StableRNGs.jl,39
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostExperimentStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningCore,#NeuralNetworkApproximator#112,false,false,0,1,"NeuralNetworkApproximator{_A,_B} where _B where _A","Any,Any,Type{NeuralNetworkApproximator}",util.jl,448
ReinforcementLearningCore,include,false,false,0,1,Any,String,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/ReinforcementLearningCore.jl,1
ReinforcementLearningCore,#_#114,false,false,0,1,Any,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},Any",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/approximators/neural_network_approximator.jl,23
ReinforcementLearningCore,update!,true,true,2,2,Nothing,"Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreEpisodeStage",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl,95
ReinforcementLearningCore,#params#117,true,true,0,1,Zygote.Params,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(Flux.params),NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl,17
ReinforcementLearningCore,select_last_dim,true,true,0,1,"SubArray{Bool,1,CircularArrayBuffers.CircularArrayBuffer{Bool,1},Tuple{Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Bool,1},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,18
ReinforcementLearningCore,get_ϵ,true,true,1,2,Float64,"EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG},Int64",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/explorers/epsilon_greedy_explorer.jl,88
ReinforcementLearningCore,select_last_frame,true,true,0,1,Int64,"CircularArrayBuffers.CircularArrayBuffer{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,21
ReinforcementLearningCore,send_to_device,false,false,0,1,Any,"Val{:gpu},Array{Float32,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,14
ReinforcementLearningCore,device,false,false,0,1,Any,"Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,22
ReinforcementLearningBase,is_body_missing,true,true,0,1,Bool,"Val{:function},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,64
ReinforcementLearning,eval,false,false,0,1,Any,Expr,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearning/OfIXm/src/ReinforcementLearning.jl,1
ReinforcementLearningEnvironments,reset!,true,false,0,1,Nothing,"CartPoleEnv{T<:Number,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl,76
ReinforcementLearningEnvironments,__init__,false,false,4,2,Any,,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/ReinforcementLearningEnvironments.jl,19
ReinforcementLearningBase,extract_name,false,false,0,1,Any,"Val{:const},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,59
ReinforcementLearningCore,update!,true,true,0,1,Nothing,"QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreActStage",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/q_based_policy.jl,57
Random.DSFMT,copy,true,true,0,1,Random.DSFMT.DSFMT_state,Random.DSFMT.DSFMT_state,/build/source/usr/share/julia/stdlib/v1.5/Random/src/DSFMT.jl,38
LinearAlgebra,copyto!,true,true,4,1,"Array{Float64,2}","Array{Float64,2},UnitRange{Int64},UnitRange{Int64},Char,Array{Float64,2},UnitRange{Int64},UnitRange{Int64}",/build/source/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl,607
ReinforcementLearningCore,_run,true,false,7,1,"ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}}","Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},StopAfterStep{ProgressMeter.Progress},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/run.jl,16
ReinforcementLearningCore,update!,true,true,0,1,Nothing,"QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreEpisodeStage",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/q_based_policy.jl,57
ReinforcementLearningBase,handle,false,false,0,1,"Tuple{Any,Bool}",Expr,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,43
ReinforcementLearningEnvironments,eval,false,false,0,1,Any,Expr,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/ReinforcementLearningEnvironments.jl,1
ReinforcementLearningCore,@E_cmd,false,false,0,1,Any,"LineNumberNode,Module,Any",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/experiment.jl,24
ReinforcementLearningCore,#QBasedPolicy#136,true,true,0,1,"QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}}","BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG},Type{QBasedPolicy}",util.jl,448
ReinforcementLearningCore,update!,true,true,0,1,Nothing,"NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},Zygote.Grads",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/approximators/neural_network_approximator.jl,33
ReinforcementLearningCore,device,false,false,0,1,Any,"Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,22
ReinforcementLearningCore,#NeuralNetworkApproximator#112,true,true,0,1,"NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM}","Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM,Type{NeuralNetworkApproximator}",util.jl,448
ReinforcementLearningCore,select_last_dim,true,true,0,1,"SubArray{Float32,1,CircularArrayBuffers.CircularArrayBuffer{Float32,1},Tuple{Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Float32,1},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,18
ReinforcementLearningBase,extract_name,false,false,0,1,Any,"Val{:<:},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,53
ReinforcementLearningCore,#EpsilonGreedyExplorer#127,false,false,0,1,"EpsilonGreedyExplorer{_A,_B,StableRNGs.LehmerRNG} where _B where _A","Float64,Symbol,Float64,Int64,Int64,Int64,Bool,Bool,StableRNGs.LehmerRNG,Type{EpsilonGreedyExplorer}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/explorers/epsilon_greedy_explorer.jl,53
ReinforcementLearningBase,reset!,true,false,0,1,Nothing,"CartPoleEnv{_A,StableRNGs.LehmerRNG} where _A",none,0
ReinforcementLearningBase,extract_name,false,false,0,1,Any,"Val{:struct},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,58
ReinforcementLearningCore,#consecutive_view#5,true,true,0,1,"SubArray{Bool,1,CircularArrayBuffers.CircularArrayBuffer{Bool,1},Tuple{Array{Int64,1}},false}","Nothing,Nothing,typeof(consecutive_view),CircularArrayBuffers.CircularArrayBuffer{Bool,1},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,109
CUDA,device,true,true,3,1,CUDA.CuDevice,,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/CUDA/mbPFj/src/state.jl,224
ReinforcementLearningCore,device,true,true,0,1,Nothing,NamedTuple,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,40
ReinforcementLearningBase,extract_name,false,false,0,1,Any,"Val{:call},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,51
ReinforcementLearningCore,device,true,false,0,1,Val{:cpu},Array,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,25
ReinforcementLearningCore,#DoEveryNEpisode#171,true,true,0,1,"DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}","PostEpisodeStage,Type{DoEveryNEpisode},ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode},Int64,Int64",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,270
ReinforcementLearningCore,send_to_host,false,false,0,1,Any,"Array{Float32,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,11
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostActStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningCore,#TotalRewardPerEpisode#161,true,true,0,1,TotalRewardPerEpisode,"Array{Float64,1},Float64,Type{TotalRewardPerEpisode}",util.jl,438
ReinforcementLearningCore,#keys#51,true,true,0,1,"NTuple{4,Symbol}","Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(keys),Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl,17
ReinforcementLearningCore,consecutive_view,true,true,0,1,"SubArray{Float32,2,CircularArrayBuffers.CircularArrayBuffer{Float32,2},Tuple{Base.Slice{Base.OneTo{Int64}},Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Float32,2},Array{Int64,1},Nothing,Nothing",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,111
ReinforcementLearningCore,device,false,false,1,2,Any,"Tuple{Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,40
ReinforcementLearningZoo,update!,true,false,4,1,Nothing,"BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},NamedTuple{(:state, :action, :reward, :terminal, :next_state),Tuple{Array{Float32,2},Array{Int64,1},Array{Float32,1},Array{Bool,1},Array{Float32,2}}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl,69
ReinforcementLearningEnvironments,action_space,true,true,0,1,Base.OneTo{Int64},"CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl,84
ReinforcementLearningCore,device,true,true,1,2,"Union{Val{:cpu}, Nothing}","NamedTuple{(:b, :σ),Tuple{Array{Float32,1},typeof(NNlib.relu)}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,40
ReinforcementLearningCore,fetch!,false,false,1,2,Any,"BatchSampler{(:state, :action, :reward, :terminal, :next_state)},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl,88
ReinforcementLearningCore,device,true,true,0,1,Val{:cpu},"Array{Float32,2}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,25
ReinforcementLearningBase,interfacem,true,false,2,3,Expr,"Module,LineNumberNode,Expr,Array{Any,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,23
ReinforcementLearningCore,check,true,true,0,1,Nothing,"BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/run.jl,14
ReinforcementLearningCore,send_to_host,false,false,0,1,Any,Any,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,11
ReinforcementLearningZoo,include,false,false,0,1,Any,String,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/ReinforcementLearningZoo.jl,1
ReinforcementLearningCore,#Agent#102,false,false,0,1,Agent,"Any,Any,Type{Agent}",util.jl,448
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreActStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},Any",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningCore,update!,true,true,0,1,Nothing,"BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostActStage",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/abstract_learner.jl,22
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreEpisodeStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningCore,device,true,true,0,1,Nothing,Tuple{},/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,26
ReinforcementLearningZoo,update!,true,false,1,2,Nothing,"BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl,62
ReinforcementLearningCore,update!,true,false,3,1,Nothing,"Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreActStage,Any",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl,110
ReinforcementLearningCore,consecutive_view,false,false,0,1,Any,"CircularArrayBuffers.CircularArrayBuffer,Array{Int64,1},Nothing,Nothing",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,111
ReinforcementLearningCore,device,false,false,1,2,Any,"Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,40
ReinforcementLearningBase,is_body_missing,true,true,0,1,Bool,"Val{:abstract},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,64
ReinforcementLearningEnvironments,state,true,true,0,1,"Array{Float32,1}","CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl,97
ReinforcementLearningCore,update!,true,true,0,1,Nothing,"BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreActStage",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/abstract_learner.jl,29
ReinforcementLearningCore,device,false,false,1,2,Any,"NamedTuple{(:Q,),Tuple{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM}}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,40
ReinforcementLearningEnvironments,reward,true,true,1,2,Float32,"CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl,95
ReinforcementLearningZoo,#Experiment#368,true,false,1,1,Experiment,"Int64,Nothing,Type{Experiment},Val{:JuliaRL},Val{:BasicDQN},Val{:CartPole},Nothing",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/experiments/rl_envs/JuliaRL_BasicDQN_CartPole.jl,1
ReinforcementLearningZoo,#BasicDQNLearner#51,false,false,0,1,"BasicDQNLearner{_A,_B,_C} where _C where _B where _A","Any,Any,Float32,Any,Any,Any,Type{BasicDQNLearner}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl,43
ReinforcementLearningEnvironments,is_terminated,true,true,0,1,Bool,"CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl,96
ReinforcementLearningCore,send_to_device,false,false,0,1,Any,"Val{:cpu},Any",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,13
ReinforcementLearningCore,#device#118,false,false,0,1,Any,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(device),NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl,17
ReinforcementLearningBase,legal_action_space_mask,true,true,0,0,Union{},"CartPoleEnv{Float32,StableRNGs.LehmerRNG}",none,0
ReinforcementLearningCore,consecutive_view,true,true,0,1,"SubArray{Bool,1,CircularArrayBuffers.CircularArrayBuffer{Bool,1},Tuple{Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Bool,1},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,109
ReinforcementLearningBase,legal_action_space_mask,true,true,0,0,Union{},"CartPoleEnv{Float32,StableRNGs.LehmerRNG},DefaultPlayer",none,0
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostExperimentStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningCore,fetch!,false,false,1,2,Any,"BatchSampler{(:state, :action, :reward, :terminal, :next_state)},Trajectory{var""#s57""} where var""#s57""<:(NamedTuple{(:state, :action, :reward, :terminal),var""#s16""} where var""#s16""<:(Tuple{var""#s15"",var""#s14"",var""#s12"",var""#s84""} where var""#s84""<:CircularArrayBuffers.CircularArrayBuffer where var""#s12""<:CircularArrayBuffers.CircularArrayBuffer where var""#s14""<:CircularArrayBuffers.CircularArrayBuffer where var""#s15""<:CircularArrayBuffers.CircularArrayBuffer)),Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl,88
ReinforcementLearningBase,extract_name,false,false,0,1,Any,"Val{:curly},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,54
ReinforcementLearningCore,consecutive_view,true,true,0,1,"SubArray{Int64,1,CircularArrayBuffers.CircularArrayBuffer{Int64,1},Tuple{Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Int64,1},Array{Int64,1},Nothing,Nothing",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,111
CommonRLInterface,include,false,false,0,1,Any,String,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/CommonRLInterface/aRBRq/src/CommonRLInterface.jl,1
Flux,glorot_uniform,false,false,0,1,Any,"StableRNGs.LehmerRNG,Int64,Vararg{Int64,N} where N",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/Flux/goUGu/src/utils.jl,65
ReinforcementLearningCore,get_ϵ,true,true,1,2,Float64,"EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/explorers/epsilon_greedy_explorer.jl,98
ReinforcementLearningCore,sample,false,false,0,1,"Tuple{Array{Int64,1},Any}","StableRNGs.LehmerRNG,Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},BatchSampler{(:state, :action, :reward, :terminal, :next_state)}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl,71
ReinforcementLearningCore,#_#76,true,true,0,1,"BatchSampler{(:state, :action, :reward, :terminal, :next_state)}","Nothing,Random._GLOBAL_RNG,Type{BatchSampler{(:state, :action, :reward, :terminal, :next_state)}},Int64",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl,65
ReinforcementLearningCore,consecutive_view,true,true,0,1,"SubArray{Int64,1,CircularArrayBuffers.CircularArrayBuffer{Int64,1},Tuple{Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Int64,1},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,109
Flux,glorot_uniform,true,true,0,1,"Array{Float32,2}","StableRNGs.LehmerRNG,Int64,Int64",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/Flux/goUGu/src/utils.jl,65
ReinforcementLearningZoo,eval,false,false,0,1,Any,Expr,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/ReinforcementLearningZoo.jl,1
ReinforcementLearningBase,extract_name,false,false,0,1,Any,"Val{:(=)},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,50
ReinforcementLearningCore,#consecutive_view#5,true,true,0,1,"SubArray{Int64,1,CircularArrayBuffers.CircularArrayBuffer{Int64,1},Tuple{Array{Int64,1}},false}","Nothing,Nothing,typeof(consecutive_view),CircularArrayBuffers.CircularArrayBuffer{Int64,1},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,109
Flux,glorot_uniform,false,false,0,1,Any,"Integer,Integer",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/Flux/goUGu/src/utils.jl,66
ReinforcementLearningCore,check,true,true,0,1,Nothing,"EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG},CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/run.jl,14
ReinforcementLearningCore,#getindex#50,false,false,0,1,CircularArrayBuffers.CircularArrayBuffer,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},typeof(getindex),Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},Symbol",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl,17
ReinforcementLearningCore,device,true,true,0,1,Val{:cpu},"Array{Float32,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,25
LoopVectorization,copyto!,true,true,2,1,"Union{Nothing, LoopVectorization.Operation}","LoopVectorization.LoopSet,Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/LoopVectorization/s8Gx6/src/constructors.jl,13
ReinforcementLearningCore,update!,true,true,0,1,Nothing,"QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostEpisodeStage",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/q_based_policy.jl,57
ReinforcementLearningCore,select_last_dim,false,false,0,1,Any,"CircularArrayBuffers.CircularArrayBuffer,Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,18
ReinforcementLearningCore,check,true,true,2,1,Nothing,"Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl,26
ReinforcementLearningEnvironments,#CartPoleEnv#40,false,false,0,1,"CartPoleEnv{_A,StableRNGs.LehmerRNG} where _A","Type{T} where T,Float64,Float64,Float64,Float64,Float64,Int64,Float64,StableRNGs.LehmerRNG,Type{CartPoleEnv}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl,45
ReinforcementLearningCore,#_#114,true,true,0,1,"Array{Float32,2}","Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},Array{Float32,2}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/approximators/neural_network_approximator.jl,23
ReinforcementLearningBase,is_body_missing,true,true,0,1,Bool,Expr,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,63
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostActStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningCore,select_last_dim,true,true,0,1,Int64,"CircularArrayBuffers.CircularArrayBuffer{Int64,1},Int64",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,18
ReinforcementLearningBase,include,false,false,0,1,Any,String,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/ReinforcementLearningBase.jl,1
ReinforcementLearningBase,is_body_missing,true,true,0,1,Bool,"Val{:const},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,64
Pkg.Types,copy,true,true,0,1,Pkg.Types.VersionSpec,Pkg.Types.VersionSpec,/build/source/usr/share/julia/stdlib/v1.5/Pkg/src/versions.jl,230
ReinforcementLearningZoo,#BasicDQNLearner#51,true,true,0,1,"BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG}","NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),Float32,Int64,Int64,StableRNGs.LehmerRNG,Type{BasicDQNLearner}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl,43
ReinforcementLearningCore,update!,true,true,1,1,"CircularArrayBuffers.CircularArrayBuffer{Bool,1}","Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostActStage",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl,153
ReinforcementLearningCore,update!,true,true,0,1,Nothing,"QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostActStage",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/q_based_policy.jl,57
ReinforcementLearningCore,consecutive_view,true,true,0,1,"SubArray{Float32,2,CircularArrayBuffers.CircularArrayBuffer{Float32,2},Tuple{Base.Slice{Base.OneTo{Int64}},Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Float32,2},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,109
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreActStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningCore,#Agent#102,true,true,0,1,"Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}}","QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},Type{Agent}",util.jl,448
ReinforcementLearningCore,send_to_device,false,false,0,1,Any,"Val{:gpu},NamedTuple{(:state, :action, :reward, :terminal, :next_state),Tuple{Array{Float32,2},Array{Int64,1},Array{Float32,1},Array{Bool,1},Array{Float32,2}}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,14
ReinforcementLearningCore,#CircularArrayTrajectory#52,false,false,0,1,Trajectory{_A} where _A,"Int64,Base.Iterators.Pairs{Symbol,Pair{DataType,Tuple{}},Tuple{Symbol,Symbol},NamedTuple{(:reward, :terminal),Tuple{Pair{DataType,Tuple{}},Pair{DataType,Tuple{}}}}},typeof(CircularArrayTrajectory)",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory.jl,44
ReinforcementLearningBase,is_body_missing,true,true,0,1,Bool,"Val{:call},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,65
ReinforcementLearningCore,device,false,false,0,1,Any,"NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/MacroTools/gME9C/src/examples/forward.jl,17
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PostEpisodeStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningBase,extract_name,false,false,0,1,Any,QuoteNode,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,47
ReinforcementLearningCore,update!,true,true,0,1,Nothing,"BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PostEpisodeStage",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/abstract_learner.jl,22
CommonRLInterface.Wrappers,include,false,false,0,1,Any,String,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/CommonRLInterface/aRBRq/src/wrappers.jl,1
ReinforcementLearningCore,device,true,true,0,1,Nothing,typeof(NNlib.relu),/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,23
ReinforcementLearningEnvironments,#CartPoleEnv#40,false,false,0,1,"CartPoleEnv{_A,StableRNGs.LehmerRNG} where _A","DataType,Float64,Float64,Float64,Float64,Float64,Int64,Float64,StableRNGs.LehmerRNG,Type{CartPoleEnv}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl,45
ReinforcementLearningCore,check,true,true,3,1,Nothing,"QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/q_based_policy.jl,66
ReinforcementLearningZoo,__init__,false,false,1,2,Any,,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/ReinforcementLearningZoo.jl,36
ReinforcementLearningBase,extract_name,false,false,0,1,Any,"Val{:(::)},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,52
CommonRLInterface.Wrappers,@forward_to_wrapped,true,false,0,1,Expr,"LineNumberNode,Module,Any",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/CommonRLInterface/aRBRq/src/wrappers.jl,42
ReinforcementLearningCore,device,false,false,1,2,Any,"NamedTuple{(:Q,),_A} where _A<:Tuple",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,40
ReinforcementLearningCore,device,false,false,0,1,Any,"Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,22
ReinforcementLearningCore,consecutive_view,true,true,0,1,"SubArray{Float32,1,CircularArrayBuffers.CircularArrayBuffer{Float32,1},Tuple{Array{Int64,1}},false}","CircularArrayBuffers.CircularArrayBuffer{Float32,1},Array{Int64,1},Nothing,Nothing",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,111
ReinforcementLearningCore,#CircularArrayTrajectory#52,false,false,0,1,Trajectory{_A} where _A,"Int64,Base.Iterators.Pairs{Symbol,Pair{DataType,B} where B,Tuple{Symbol,Symbol},NamedTuple{(:state, :action),Tuple{Pair{DataType,Tuple{Int64}},Pair{DataType,Tuple{}}}}},typeof(CircularArrayTrajectory)",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory.jl,44
ReinforcementLearningEnvironments,include,false,false,0,1,Any,String,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/ReinforcementLearningEnvironments.jl,1
ReinforcementLearningBase,@multi_agent_env_api,true,false,0,1,Expr,"LineNumberNode,Module,Any",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,19
CommonRLInterface.Wrappers,@quick_forward,true,false,0,1,Expr,"LineNumberNode,Module,Any",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/CommonRLInterface/aRBRq/src/quick_wrapper.jl,32
ReinforcementLearningCore,update!,true,true,3,1,Nothing,"Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}},QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},CartPoleEnv{Float32,StableRNGs.LehmerRNG},PreActStage,Int64",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/agent.jl,110
ReinforcementLearningZoo,update!,true,false,1,2,Nothing,"BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},AbstractTrajectory",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningZoo/M308M/src/algorithms/dqns/basic_dqn.jl,62
ReinforcementLearningCore,device,false,false,0,1,Any,"BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,22
Flux,glorot_uniform,true,true,0,1,"Flux.var""#1#2""{StableRNGs.LehmerRNG}",StableRNGs.LehmerRNG,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/Flux/goUGu/src/utils.jl,67
ReinforcementLearningCore,sample,false,false,0,1,"Tuple{Any,Any}","StableRNGs.LehmerRNG,AbstractTrajectory,BatchSampler",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory_extension.jl,71
ReinforcementLearningBase,ActionStyle,true,true,0,1,MinimalActionSet,"CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/interface.jl,322
ReinforcementLearningEnvironments,#CartPoleEnv#40,true,true,0,1,"CartPoleEnv{Float32,StableRNGs.LehmerRNG}","Type{Float32},Float64,Float64,Float64,Float64,Float64,Int64,Float64,StableRNGs.LehmerRNG,Type{CartPoleEnv}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl,45
ReinforcementLearningCore,#CircularArraySARTTrajectory#58,false,false,0,1,Trajectory{_A} where _A,"Int64,Pair{DataType,Tuple{Int64}},Pair{DataType,Tuple{}},Pair{DataType,Tuple{}},Pair{DataType,Tuple{}},Type{Trajectory{var""#s57""} where var""#s57""<:(NamedTuple{(:state, :action, :reward, :terminal),var""#s16""} where var""#s16""<:(Tuple{var""#s15"",var""#s14"",var""#s12"",var""#s84""} where var""#s84""<:CircularArrayBuffers.CircularArrayBuffer where var""#s12""<:CircularArrayBuffers.CircularArrayBuffer where var""#s14""<:CircularArrayBuffers.CircularArrayBuffer where var""#s15""<:CircularArrayBuffers.CircularArrayBuffer))}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/agents/trajectories/trajectory.jl,76
ReinforcementLearningCore,#consecutive_view#5,true,true,0,1,"SubArray{Float32,1,CircularArrayBuffers.CircularArrayBuffer{Float32,1},Tuple{Array{Int64,1}},false}","Nothing,Nothing,typeof(consecutive_view),CircularArrayBuffers.CircularArrayBuffer{Float32,1},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,109
ReinforcementLearningBase,is_body_missing,true,true,0,1,Bool,"Val{:struct},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,64
ReinforcementLearningCore,device,true,true,1,2,"Union{Val{:cpu}, Nothing}","NamedTuple{(:W, :b, :σ),Tuple{Array{Float32,2},Array{Float32,1},typeof(NNlib.relu)}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,40
ReinforcementLearningCore,#_#157,true,false,4,1,Nothing,"Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},ComposedHook{Tuple{TotalRewardPerEpisode,TimePerStep,DoEveryNStep{ReinforcementLearningZoo.var""#369#373""{TensorBoardLogger.TBLogger}},DoEveryNEpisode{PostEpisodeStage,ReinforcementLearningZoo.var""#371#375""{TensorBoardLogger.TBLogger,TotalRewardPerEpisode}}}},PreExperimentStage,Agent{QBasedPolicy{BasicDQNLearner{NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},typeof(Flux.Losses.huber_loss),StableRNGs.LehmerRNG},EpsilonGreedyExplorer{:exp,false,StableRNGs.LehmerRNG}},Trajectory{NamedTuple{(:state, :action, :reward, :terminal),Tuple{CircularArrayBuffers.CircularArrayBuffer{Float32,2},CircularArrayBuffers.CircularArrayBuffer{Int64,1},CircularArrayBuffers.CircularArrayBuffer{Float32,1},CircularArrayBuffers.CircularArrayBuffer{Bool,1}}}}},Vararg{Any,N} where N",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/core/hooks.jl,45
ReinforcementLearningCore,device,true,true,0,1,Nothing,"NamedTuple{(),Tuple{}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,27
ReinforcementLearningBase,@env_api,true,false,0,1,Expr,"LineNumberNode,Module,Any",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,15
ReinforcementLearningEnvironments,reset!,true,true,0,1,Nothing,"CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningEnvironments/xB2IU/src/environments/examples/CartPoleEnv.jl,76
AbstractTrees,include,false,false,0,1,Any,String,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/AbstractTrees/oHb1F/src/AbstractTrees.jl,7
ReinforcementLearningBase,extract_name,true,true,0,1,Symbol,Symbol,/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,46
LinearAlgebra,copyto!,true,true,4,1,"Array{Float32,2}","Array{Float32,2},UnitRange{Int64},UnitRange{Int64},Char,Array{Float32,2},UnitRange{Int64},UnitRange{Int64}",/build/source/usr/share/julia/stdlib/v1.5/LinearAlgebra/src/matmul.jl,607
ReinforcementLearningCore,#_#114,true,true,0,1,"Array{Float32,1}","Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},NeuralNetworkApproximator{Flux.Chain{Tuple{Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(NNlib.relu),Array{Float32,2},Array{Float32,1}},Flux.Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}}},Flux.Optimise.ADAM},Array{Float32,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/policies/q_based_policies/learners/approximators/neural_network_approximator.jl,23
ReinforcementLearningBase,extract_name,false,false,0,1,Any,"Val{:where},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,56
ReinforcementLearningBase,current_player,true,true,0,1,DefaultPlayer,"CartPoleEnv{Float32,StableRNGs.LehmerRNG}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/interface.jl,476
ReinforcementLearningCore,device,true,true,1,2,"Union{Val{:cpu}, Nothing}","NamedTuple{(:W, :b, :σ),Tuple{Array{Float32,2},Array{Float32,1},typeof(identity)}}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,40
ReinforcementLearningBase,extract_name,false,false,0,1,Any,"Val{:function},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,57
ReinforcementLearningBase,is_body_missing,true,true,0,1,Bool,"Val{:(=)},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,64
ReinforcementLearningCore,device,true,true,0,1,Nothing,typeof(identity),/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/device.jl,23
ReinforcementLearningCore,consecutive_view,false,false,0,1,Any,"CircularArrayBuffers.CircularArrayBuffer,Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,109
ReinforcementLearningBase,extract_name,false,false,0,1,Any,"Val{:abstract},Expr",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningBase/PGdVt/src/inline_export.jl,49
ReinforcementLearningCore,#consecutive_view#5,true,true,0,1,"SubArray{Float32,2,CircularArrayBuffers.CircularArrayBuffer{Float32,2},Tuple{Base.Slice{Base.OneTo{Int64}},Array{Int64,1}},false}","Nothing,Nothing,typeof(consecutive_view),CircularArrayBuffers.CircularArrayBuffer{Float32,2},Array{Int64,1}",/data/artem/stability/repo/Stability/pkgs/fresh6-10pkgs-ver/ReinforcementLearning/depot/packages/ReinforcementLearningCore/NWrFY/src/utils/base.jl,109
ReinforcementLearningCore,#QBasedPolicy#136,false,false,0,1,QBasedPolicy,"Any,Any,Type{QBasedPolicy}",util.jl,448
